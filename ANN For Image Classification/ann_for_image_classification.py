# -*- coding: utf-8 -*-
"""ANN for Image Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rQJW91hVr2NkGGH5sfNpF6iUL_dzebps

# Step 1: Installation and Setup
"""

!pip install --upgrade pip setuptools

!pip install tensorflow

import tensorflow as tf

print(tf.__version__)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

"""# Step 2: Data preprocessing"""

# importing dataset
from tensorflow.keras.datasets import fashion_mnist

# loading the dataset
(x_train, y_train),(x_test, y_test) = fashion_mnist.load_data()

x_train.shape, x_test.shape

y_train.shape, y_test.shape

x_train

# maximum value
np.max(x_train)

np.min(x_train), np.mean(x_train)

np.max(y_train),np.min(y_train), np.mean(y_train)

"""we have pixel from 0 to 255 (x value max and min )and classes from 0 to 9 (y value max and min)"""

# Entering all the classes
class_names = ['0 Top/T-shirt' , '1 Trouser' , '2 Pullover' , ' 3 Dress', '4 Coat', '5 Sandal', '6 Shirt', '7 Sneaker' , '8 bag', '9 Ankle boot']
print(class_names)

# Data exploration
plt.figure()
plt.imshow(x_train[1])

# adding colorbar which is from 0 to 255
plt.colorbar()

# to confirm the above
# find y_train 1 and then check the result int he above class

y_train[1]

"""0 is the class of top/t-shirt therefore this is true

In x_train we have data and in y_train we have labels
"""

# Normalizing the dataset
# after normalization the model run faster

x_train = x_train / 255.0
x_test = x_test / 255.0

# Data exploration
plt.figure()
plt.imshow(x_train[1])

# adding colorbar which is from 0 to 255
plt.colorbar()

"""After normalization the bar is from 0 to 1"""

# Flattening the dataset
# converting 60k 2D arrays into 60k 1D arrays

x_train.shape, x_test.shape

"""Converting the 28*28 dataset into 1D vector"""

x_train = x_train.reshape(-1,28*28)
x_test = x_test.reshape(-1,28*28)

"""We have done this because artificial neural networks takes input in this way"""

x_train.shape, x_test.shape

"""# Step 3: Building the Model"""

# Defining an object
model = tf.keras.models.Sequential()
# Sequence of layers

# Adding first fully connected hidden layer
# 1) units (No of neurons) = 128
# 2) activation function as ReLU (Rectified Linear Unit)
# 3) input shape = 784 (Because we have flatten the dataset)

# to add first layer to our model we have to use .add method
model.add(tf.keras.layers.Dense(units= 128, activation='relu', input_shape=(784,)))

# adding input shape as 784, to show it as vector

# Adding the second layer with dropout
# to stop the overfitting
# it is a regularisation technique
model.add(tf.keras.layers.Dropout(0.3))

# adding the output layer
# have to add only 2 parameters
# 1) units(no of neurons) = 10 (because only 10 class are outputs )
# 2) activation function = softmax (because we want multiple outputs)

# for binary output we use sigmoid activation function

model.add(tf.keras.layers.Dense(units=10, activation= 'softmax'))

"""# Step 4: Training the model"""

# Compiling the model
# 1) Optimizer = adam, (it minimizes the loss function)
# 2) loss function = sparse_categorical_crossentropy, (acts as guide to optimizer)
# 3) matrices = sparse_categorical_accuracy (to check the accuracy)

# for binary output we use matrices as accuracy

model.compile(optimizer='adam',loss= 'sparse_categorical_crossentropy', metrics =['sparse_categorical_accuracy'])

model.summary()

# Trainig the model
# In x_train we have images and in y_train we have labels of that images
# epochs means the number of times we are going to train our model

model.fit(x_train, y_train, epochs=10)

"""# Step 5: Model Evaluation and Prediction"""

# Model evaluation
test_loss, test_accuracy = model.evaluate(x_test,y_test)

"""We are know checking the accuracy on the test data and it is showing accuracy nearly equal to that on train data , this means our model model is working fine."""

# Model Prediction
# model.predict(x_test) gives you the probability distributions for each class
y_pred = model.predict(x_test)

# np.argmax(y_pred, axis=1) gives you the class with the highest probability,
# which is the equivalent of predict_classes
y_pred_classes = np.argmax(y_pred, axis=1)

print(y_pred)

print(y_pred_classes)

y_pred_classes[0]

y_test[0]

"""The above y_pred_classes is the predicted output and y_ test equals 9 is real output which is correct in this case means the model is working fine"""

y_pred_classes[110], y_test[110]

# Confusion matrix
from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test,y_pred_classes)
print(cm)

"""The Diagonal elements are the correct predictions and other elements are the wrong predictions"""

# using this confusion matrix calculating the accuracy

acc_cm = accuracy_score(y_test,y_pred_classes)
print(acc_cm)

